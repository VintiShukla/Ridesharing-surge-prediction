# -*- coding: utf-8 -*-
"""ride-surge-prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12HprbvKjwJfnQrzbs8jcLqS0GKFUE1sY

NCR Ride Bookings Preprocessing
"""

pip install mlflow optuna joblib lightgbm shap

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import lightgbm as lgb
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import mean_squared_error
import mlflow
import mlflow.lightgbm
import mlflow.xgboost
import optuna
import joblib
import logging
import shap

# ---------------------------- CONFIG ----------------------------------
CONFIG = {
    "ride_data_path": "ncr_ride_bookings.csv",
    "weather_data_path": "kaggel_weather_2013_to_2024.csv",
    "surge_threshold_quantile": 0.75,
    "random_state": 42,
    "test_size": 0.2
}

logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)

# ---------------------------- DATA PREPROCESSING -----------------------
def preprocess_ride_data(path):
    logger.info("Loading and preprocessing ride data...")
    df = pd.read_csv(path)
    df.drop(["Booking ID", "Customer ID"], axis=1, inplace=True)

    df['Original_Date'] = pd.to_datetime(df['Date'])
    df['Datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])
    df['Hour'] = df['Datetime'].dt.hour
    df['Day'] = df['Datetime'].dt.day
    df['Month'] = df['Datetime'].dt.month
    df['Weekday'] = df['Datetime'].dt.dayofweek
    df['Is_Weekend'] = df['Weekday'].isin([5, 6]).astype(int)
    df.drop(['Date', 'Time', 'Datetime'], axis=1, inplace=True)

    for col in ['Avg VTAT', 'Avg CTAT', 'Booking Value', 'Ride Distance']:
        df[col] = df.groupby('Vehicle Type')[col].transform(lambda x: x.fillna(x.median()))
    for col in ['Driver Ratings', 'Customer Rating']:
        df[col] = df.groupby('Vehicle Type')[col].transform(lambda x: x.fillna(x.mean()))

    df['Cancelled Rides by Customer'] = df['Cancelled Rides by Customer'].fillna(0)
    df['Cancelled Rides by Driver'] = df['Cancelled Rides by Driver'].fillna(0)
    df['Incomplete Rides'] = df['Incomplete Rides'].fillna(0)
    df['Reason for cancelling by Customer'] = df['Reason for cancelling by Customer'].fillna("No Cancellation")
    df['Driver Cancellation Reason'] = df['Driver Cancellation Reason'].fillna("No Cancellation")
    df['Incomplete Rides Reason'] = df['Incomplete Rides Reason'].fillna("Completed")
    df['Payment Method'] = df['Payment Method'].fillna("Unknown")

    fare_threshold = df['Booking Value'].quantile(CONFIG["surge_threshold_quantile"])
    df['Is_Surge'] = (df['Booking Value'] > fare_threshold).astype(int)

    le = LabelEncoder()
    string_cat_cols = ['Vehicle Type', 'Pickup Location', 'Drop Location', 'Payment Method']
    for col in string_cat_cols:
        df[col + '_Encoded'] = le.fit_transform(df[col])
        df.drop(col, axis=1, inplace=True)

    other_cat_cols = ['Reason for cancelling by Customer', 'Driver Cancellation Reason',
                      'Incomplete Rides Reason', 'Booking Status', 'Is_Surge']
    for col in other_cat_cols:
        df[col + '_Encoded'] = le.fit_transform(df[col].astype(str))
        df.drop(col, axis=1, inplace=True)

    logger.info(f"Ride data preprocessing complete. Shape: {df.shape}")
    return df

def preprocess_weather_data(path):
    logger.info("Loading and cleaning weather data...")
    weather_df = pd.read_csv(path)
    if 'Unnamed: 0' in weather_df.columns:
        weather_df.drop(columns=['Unnamed: 0'], inplace=True)
    weather_df['DATE'] = pd.to_datetime(weather_df['DATE'])
    weather_df.fillna(method='ffill', inplace=True)
    weather_df.columns = weather_df.columns.str.strip().str.lower().str.replace(' ', '_')
    logger.info(f"Weather data cleaned. Shape: {weather_df.shape}")
    return weather_df

def merge_and_scale(df, weather_df):
    logger.info("Merging ride and weather data...")
    weather_2024 = weather_df[weather_df['year'] == 2024].copy()
    merged_df = pd.merge(df, weather_2024, left_on='Original_Date', right_on='date', how='left')
    merged_df.drop(columns=['date'], inplace=True)

    weather_columns = [c for c in weather_df.columns if c not in ['date']]
    merged_df.sort_values(by='Original_Date', inplace=True)
    merged_df[weather_columns] = merged_df[weather_columns].ffill()

    num_cols = merged_df.select_dtypes(include=['int64', 'float64']).columns.tolist()
    exclude_from_scaling = ['year', 'month_y', 'day_y']
    num_cols = [col for col in num_cols if col not in exclude_from_scaling]

    scaler = StandardScaler()
    merged_df[num_cols] = scaler.fit_transform(merged_df[num_cols])

    joblib.dump(scaler, "scaler.pkl")
    logger.info("Scaling complete and scaler saved as scaler.pkl")

    merged_df.to_csv('preprocessed_ride_data.csv', index=False)
    logger.info(f"Merged preprocessed data saved. Shape: {merged_df.shape}")
    return merged_df

def resample_data(df, target_column, resample_factor=3):
    """
    Resamples the training data to give more weight to high-value targets.
    """
    df_copy = df.copy()
    q90 = df_copy[target_column].quantile(0.90)
    high_value_subset = df_copy[df_copy[target_column] > q90]
    resampled_data = pd.concat([df_copy] + [high_value_subset] * resample_factor, ignore_index=True)
    logger.info(f"Original training data size: {df_copy.shape[0]}, Resampled size: {resampled_data.shape[0]}")
    return resampled_data

# ---------------------------- MODEL TRAINING ---------------------------
def train_lightgbm(df):
    logger.info("Training LightGBM model...")

    X = df.drop(['Booking Value', 'Original_Date'], axis=1)
    y = df['Booking Value']

    # Handle any object columns that might have come from weather (like 'conditions')
    object_cols = X.select_dtypes(include=['object']).columns
    if len(object_cols) > 0:
        logger.warning(f"Encoding object columns: {list(object_cols)}")
        for col in object_cols:
            X[col] = X[col].astype('category').cat.codes

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=CONFIG["test_size"], random_state=CONFIG["random_state"]
    )

    def objective(trial):
        params = {
            'objective': 'regression_l1',
            'metric': 'rmse',
            'n_estimators': trial.suggest_int('n_estimators', 500, 2000),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'num_leaves': trial.suggest_int('num_leaves', 20, 300),
            'max_depth': trial.suggest_int('max_depth', 3, 12),
            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'random_state': CONFIG["random_state"],
            'n_jobs': -1,
        }
        model = lgb.LGBMRegressor(**params)
        model.fit(X_train, y_train, eval_set=[(X_test, y_test)],
                  callbacks=[lgb.early_stopping(100, verbose=False)])
        preds = model.predict(X_test)
        rmse = np.sqrt(mean_squared_error(y_test, preds))
        return rmse

    # --------------------- Hyperparameter Tuning ---------------------
    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=50)
    best_params = study.best_params

    # --------------------- Model Training ---------------------
    mlflow.set_experiment("Ride Surge Prediction")
    with mlflow.start_run(run_name="Tuned LightGBM"):
        mlflow.log_params(best_params)

        model = lgb.LGBMRegressor(**best_params, random_state=CONFIG["random_state"])
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        rmse = np.sqrt(mean_squared_error(y_test, preds))
        mlflow.log_metric("rmse", rmse)
        mlflow.lightgbm.log_model(model, "lgbm-ride-surge-model")

        # --------------------- Explainable AI (SHAP) ---------------------
        logger.info("Generating SHAP explainability plots...")
        import shap
        import matplotlib.pyplot as plt

        # Create SHAP explainer
        explainer = shap.TreeExplainer(model)
        shap_values = explainer.shap_values(X_test)

        # Global Feature Importance Plot
        shap.summary_plot(shap_values, X_test, plot_type="bar", show=False)
        plt.title("Feature Importance (SHAP)")
        plt.tight_layout()
        plt.savefig("shap_summary_bar.png", dpi=200)
        plt.close()

        # Detailed Beeswarm Plot
        shap.summary_plot(shap_values, X_test, show=False)
        plt.title("SHAP Feature Impact (Beeswarm)")
        plt.tight_layout()
        plt.savefig("shap_summary_beeswarm.png", dpi=200)
        plt.close()

        # Log SHAP plots in MLflow
        mlflow.log_artifact("shap_summary_bar.png")
        mlflow.log_artifact("shap_summary_beeswarm.png")

        logger.info("SHAP explainability plots generated and logged to MLflow.")

    # --------------------- Save Model ---------------------
    joblib.dump(model, "lgbm_model.pkl")
    logger.info(f"LightGBM training complete. RMSE: {rmse:.3f}. Model saved as lgbm_model.pkl")

    return model, rmse

def train_xgboost(df):
    logger.info("Training XGBoost model...")

    X = df.drop(['Booking Value', 'Original_Date'], axis=1)
    y = df['Booking Value']

    # Handle any object columns that might have come from weather (like 'conditions')
    object_cols = X.select_dtypes(include=['object']).columns
    if len(object_cols) > 0:
        logger.warning(f"Encoding object columns: {list(object_cols)}")
        for col in object_cols:
            X[col] = X[col].astype('category').cat.codes

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=CONFIG["test_size"], random_state=CONFIG["random_state"]
    )
    # --- ADDED RESAMPLING STEP ---
    train_df = pd.concat([X_train, y_train], axis=1)
    resampled_train_df = resample_data(train_df, 'Booking Value', resample_factor=5)
    X_train_resampled = resampled_train_df.drop('Booking Value', axis=1)
    y_train_resampled = resampled_train_df['Booking Value']
    # --------------------- Hyperparameter Tuning ---------------------
    def objective_xgb(trial):
        params = {
            'objective': 'reg:squarederror',
            'eval_metric': 'rmse',
            'n_estimators': trial.suggest_int('n_estimators', 500, 2500),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'max_depth': trial.suggest_int('max_depth', 3, 12),
            'subsample': trial.suggest_float('subsample', 0.5, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
            'gamma': trial.suggest_float('gamma', 0, 0.5),
            'random_state': CONFIG["random_state"],
            'n_jobs': -1,
            'tree_method': 'hist',
            'enable_categorical': True
        }
        model = xgb.XGBRegressor(**params)
        model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)
        preds = model.predict(X_test)
        rmse = np.sqrt(mean_squared_error(y_test, preds))
        return rmse

    study_xgb = optuna.create_study(direction='minimize')
    study_xgb.optimize(objective_xgb, n_trials=50)
    best_params = study_xgb.best_params

    # --------------------- Model Training ---------------------
    mlflow.set_experiment("Ride Surge Prediction")
    with mlflow.start_run(run_name="Tuned XGBoost"):
        mlflow.log_params(best_params)

        model = xgb.XGBRegressor(**best_params, random_state=CONFIG["random_state"],
                                 tree_method='hist', enable_categorical=True)
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        rmse = np.sqrt(mean_squared_error(y_test, preds))
        mlflow.log_metric("rmse", rmse)
        mlflow.xgboost.log_model(model, "xgb-ride-surge-model")

        # --------------------- Explainable AI (SHAP) ---------------------
        logger.info("Generating SHAP explainability plots for XGBoost...")
        import shap
        import matplotlib.pyplot as plt

        # Create SHAP explainer (TreeExplainer is best for XGBoost)
        explainer = shap.Explainer(model)
        shap_values = explainer(X_test)

        # Global Feature Importance (bar plot)
        shap.summary_plot(shap_values, X_test, plot_type="bar", show=False)
        plt.title("XGBoost Feature Importance (SHAP)")
        plt.tight_layout()
        plt.savefig("xgb_shap_summary_bar.png", dpi=200)
        plt.close()

        # Beeswarm (detailed impact)
        shap.summary_plot(shap_values, X_test, show=False)
        plt.title("XGBoost SHAP Feature Impact (Beeswarm)")
        plt.tight_layout()
        plt.savefig("xgb_shap_summary_beeswarm.png", dpi=200)
        plt.close()

        # Log plots in MLflow
        mlflow.log_artifact("xgb_shap_summary_bar.png")
        mlflow.log_artifact("xgb_shap_summary_beeswarm.png")

        logger.info("SHAP explainability plots for XGBoost saved and logged in MLflow.")

    # --------------------- Save Model ---------------------
    joblib.dump(model, "xgb_model.pkl")
    logger.info(f"XGBoost training complete. RMSE: {rmse:.3f}. Model saved as xgb_model.pkl")

    return model, rmse

def visualize_model_performance(model, X_test, y_test, model_name="Model"):
    """
    Visualize model predictions vs actual values and print key metrics.
    """
    import matplotlib.pyplot as plt
    from sklearn.metrics import mean_absolute_error, r2_score

    preds = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, preds))
    mae = mean_absolute_error(y_test, preds)
    r2 = r2_score(y_test, preds)

    print(f"\n📊 {model_name} Performance:")
    print(f"RMSE: {rmse:.3f}")
    print(f"MAE : {mae:.3f}")
    print(f"R²  : {r2:.3f}")

    # Scatter plot of actual vs predicted
    plt.figure(figsize=(6,6))
    plt.scatter(y_test, preds, alpha=0.4)
    plt.plot([y_test.min(), y_test.max()],
             [y_test.min(), y_test.max()],
             'r--', lw=2)
    plt.xlabel("Actual Booking Value")
    plt.ylabel("Predicted Booking Value")
    plt.title(f"{model_name} Predictions vs Actuals")
    plt.tight_layout()
    plt.show()

# ---------------------------- MAIN ------------------------------------
if __name__ == "__main__":
    ride_df = preprocess_ride_data(CONFIG["ride_data_path"])
    weather_df = preprocess_weather_data(CONFIG["weather_data_path"])
    merged_df = merge_and_scale(ride_df, weather_df)

    lgb_model, lgb_rmse = train_lightgbm(merged_df)
    xgb_model, xgb_rmse = train_xgboost(merged_df)

    logger.info(f"✅ All training complete. LGBM RMSE: {lgb_rmse:.3f}, XGB RMSE: {xgb_rmse:.3f}")

from sklearn.model_selection import train_test_split

X = merged_df.drop(['Booking Value', 'Original_Date'], axis=1)
y = merged_df['Booking Value']

# Encode any object columns if needed
object_cols = X.select_dtypes(include=['object']).columns
if len(object_cols) > 0:
    for col in object_cols:
        X[col] = X[col].astype('category').cat.codes

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
visualize_model_performance(lgb_model, X_test, y_test, "LightGBM")
visualize_model_performance(xgb_model, X_test, y_test, "XGBoost")



!zip -r ride_surge_project.zip /content
from google.colab import files
files.download("ride_surge_project.zip")

